			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

Sean Hogan <seanhogan@uchicago.edu>
Charles Cary <cioc@uchicago.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

thread.c
--------

/* The list of sleeping threads */
static struct list wait_list;

thread.h
--------

/* Used for allowing a thread to be in the wait_list */
struct list_elem waitelem;

/* A semaphore that a thread uses to put itself to sleep */
struct semaphore timer_semaphore;

/* When a thread sleeps, it should wake up at this time relative to the kernel starting. */
int64_t wakeup_time;

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

We tell the current running thread to sleep for TICKS ticks. This just changes the running thread's wakeup time to be the current time + TICKS. The running thread is inserted into "wait_list" which is sorted in ascending order by wakeup time. Then, the thread is put to sleep and blocks, via sema_init and sema_down.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We added a call to "thread.c::thread_wake_routine" , which just iterates through the sleeping threads and sees if any of them need to be woken up. This is fast and does not block.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The only issue is if we are interrupted when getting the return value of timer_ticks. So, we turn off interrupts as we query for the number of ticks since the kernel started, and then turn them off right after.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

We block critical sections of code by disabling interrupts. The critical sections are inside of timer_ticks() where we read the value of ticks. Since timer_sleep() also calls thread_sleep(), we also make sure to turn off interrupts when inserting the current thread into the wait_list.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it may be important in the future to be able to keep track of all sleeping threads based on their wake-up time. We initially considered not having a list of current sleeping threads and just looping through all threads of all states and updating the sleep metadata that way. That design is worse as it's both inefficient and doesn't allow an easy way to keep track of the order of threads to be woken up.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

thread.c
--------

/* The following methods were declared in thread.h but defined in thread.c */

/* Helper method for both priority scheduler and MLFQS */
int get_priority(struct thread *t); /* get's the priority of the thread based on 
                                       whatever scheduler the system is set to use */

/* Added for Priority Scheduling */

/* compares the priority of two threads; used for ordering */
bool thread_priority_compare (const struct list_elem *a, const struct list_elem *b, void *aux); 

/* performs a priority donation from one thread to another */
void donate_priority(struct thread *source, struct thread *target);

/* This removes priority donations from a thread with respect to a lock*/
void empty_donated_priority(struct thread *t, struct lock *lock);


thread.h
--------

/* This is the maxiumum depth for priority donation. */
#define DONATE_DEPTH 8

/* This represents a priority donation.  */
struct donor_elem {
  struct thread *t;      /* This is the thread that made the donation. */
  int donation;          /* This is the donation that the thread made. */
  struct list_elem elem; /* Used for keeping track of all donors to a thread.*/
};

/* The following were added to struct thread */

/* Stuff for Priority Donation */
int donated_priority;         /* Maintains the highest donated priority to the thread. */
struct list donor_list;       /* Keeps a list of all donations to the thread. */
struct thread *donee;         /* Which thread that this thread has donated to. */
struct lock *waiting_on_lock; /* The lock that this thread is waiting on. */

synch.h
-------

/* added to struct lock */
struct list_elem elem; /* added so that one can maintain a list of locks*/


synch.c
------

/* This method was declared in synch.h but implemented in synch.c */
/* This compared two semaphores in a condvar.  It compares the priorities of the threads that are holding/waiting on the semaphore */
bool sema_priority_compare (const struct list_elem *a, const struct list_elem *b, void *aux);

/* Added to struct semaphore_elem */
/* Needed for condvars */
struct thread *owner; /* In a list of semaphores, this keeps track of which thread added the semaphore to the semaphore list.  This is necessary for sorting the semphores based on the priorities of their owner threads. */


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

In order to make priority donation work, every thread maintains three things:
1) Its priority (priority)
2) The highest priority donated to the thread (donated_priority)
3) A list of all priority donations to the thread.

An entry in the priority donation list of a thread consists of a donor_elem structure.  A donor_elem structure keeps track of which thread made the donation and what the donation was. Entries are made via the donte_priority function.  

When a thread receives a priority donation, it increases it's donated_priority to the value of the donation if the donation is greater than donated_priority.

To calculate a thread's donation for the sake of ordering the threads, each thread's priority = max(priority, donated_priority).

When a thread acquires a lock that it has been waiting on, it may reduce its donated_priority.  This is done via the empty_donated_priority method.  This method removes all donations that come from threads that were waiting on the lock. It adjusts donated_priority to the largest remaining donation.

Priority donation occurs via the donate_priority and donate_priority_helper methods.
donate_priority starts priority donation via the donate_priority_helper method.  Donate_priority helper not only adds a donation to the target thread's donation list, it also causes the donation to cascade forward, performing the same donation to the thread which the donated-to thread is waiting on.  It does this up to 8 levels of depth.  This depth can be adjusted via the DONATE_DEPTH constant found in thread.h

And now for an example of priority donation with pictures!

Consider three threads of high(H), medium(M), and low(L) priorities. 
(~ implies waiting for a lock and = implies a lock acquired.  The lists at the head of our threads represent the donation_list of each thread.)


L and M each have acquired a lock.  

-----         -----
L   =LOCK1    M   =LOCK2
-----         -----

M begins to wait on LOCK1

-----         ----- 
L   =LOCK1    M   =LOCK2,~LOCK1
-----         -----

This causes M to donate it's priority to L

-----         -----
M,L =LOCK1    M   =LOCK2,~LOCK1
-----         -----

But H still has highest priority.  So it runs.  H tries to acquire LOCK2.  SO, it donates its priority to M.

-----         -----                 -----
M,L =LOCK1    H,M =LOCK2,~LOCK1     H   ~LOCK2
-----         -----                 -----

This donation to M triggers a cascading donation to L:

-----         -----                 -----
H,M,L=LOCK1   H,M =LOCK2,~LOCK1     H   ~LOCK2
-----         -----                 -----

L can how run since it has high priority.  It does and releases LOCK1. This causes it to empty it's donation list.

-----         -----                 -----
L   =         H,M =LOCK2,~LOCK1     H   ~LOCK2
-----         -----                 -----

This causes M to run since it to has high priority.  It now acquires LOCK1 and finishes it's computing, causing it to empty its donation list.

-----         -----                 -----
L   =         M   =                 H   ~LOCK2
-----         -----                 -----

Now only H has high priority, so it acquires LOCK2 and finally does its computing.

-----         -----                 -----
L   =         M   =                 H   =LOCK2
-----         -----                 -----

-----         -----                 -----
L   =         M   =                 H   =
-----         -----                 -----

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

For semaphores, highest priority ordering is assured because the
threads waiting on the semaphore are ordered by priority.  Instead of 
simply putting waiting threads in a waitind list, during sema_down, 
we add them in priority order.  Furthermore, as priority can change 
as our system runs, due primarily to priority donation, we also 
resort our waiting list when we unblock a thread via sema_up. 

Lock ordering nearly follows from semaphore ordering expect for a few
minor changes.  Beyond just allowing semaphores that are aware of 
priority, our locks trigger priority donation when when a thread 
waits while trying to acquire a lock.  

As for condition variables, we use the list of internal semaphores,
but we now sort this list according to the priority of the threads
that add the semaphore to the list.  This ensures ordering. 

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When lock_acquire is called and the lock is currently held by another
process, a call to donate_priority is triggered. Thsi triggers a call 
to donate_priority_helper.  Donate prioroty helper, adds a donation
to the donation list of the thread that currently holds the lock. If 
this thread is also waiting on a lock, a donation is performed on that
lock, recursively.  This recursion is allowed to go 8 layers deep.  

We then put our semaphore down and switch the lock holder to the current
thread.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

A call to lock release triggers a call to empty_donated_priority.  This
method essentially undoes priority donation. The thread releasing the 
lock empties its donation list of any donation that occured because of
waiting on the released lock and it adjusts its prioroty based on
its new donation list.  Our semaphore is then put up and the next highest
thread waiting on the lock acquires the lock.  


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

A potential race condition occurs when we begin to set a threads priority to the highest priority of the ready threads, which should cause it run, but the call
is preempted by the scheduler and another thread is selected to run before
thread_set_priority returns.  We turned off interrupts to solve this problem. 

Using a lock would be unnecessary overhead in this context.  We would have to 
have a global lock of all threads.  The code is far more understandle in its 
present manner.  

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it reduces priority donation to a a recursive call. 
Rather than account for priority donation all over the place, we can just have
one method that performs priority donation.  This, with our get_priority function,
simplifies much of the code.

Our first design did most of the priority donation in the syncronizations structures 
themselves.  This was a bad design for two reasons:

1) Stack overflows
2) separating the logic of pririty into two places

Our current implemention does not have either of these problems.  


			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

in thread.c

static int ready_list_length; //Length of the ready list - for convenient use in the timer interrupt.
static fp_t load_avg; // load average

in thread.h::thread

int nice; // Niceness of a thread.
fp_t recent_cpu; // Recent_cpu value

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

load avg and recent cpu dont change via the formula


priority = 63 - (recent/4) - nice*2
recent up by 1 for running
timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  31  31  31  A
 4      4   0   0  62  61  59  A
 8      8   0   0  61  61  59  B
12      8   4   0  61  60  59  A
16      12  4   0  60  60  59  B
20      12  8   0  60  59  59  A
24      16  8   0  59  59  59  C
28      16  8   4  59  59  58  B
32      16  12  4  59  58  58  A
36      20  12  4  58  58  58  C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

- If priorities are the same to begin, which thread starts? We just use the thread at the beginning of the queue, and it does the same as ours.

- How do we deal with round-robin when dealing with the number of threads having priority X increasing from interrupt to interrupt? We just made sure that when a thread is re-inserted to the ready list, it is pushed to the back of its subset of priorities (so if A, B have PRI 30 and C just went to ready from running, then the queue will be A,B,C, so A runs next (assuming no other threads have PRI > 30)). This matches the behavior of our scheduler.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

- The code to update all of the parameters for MLFQS happen in external interrupt context (during the timer interrupt) so that means they are called every timer interrupt. All other schedule code (donating, etc.) is called from an internal interrupt context so we switch off when needed. By keeping only the logic for updating MLFQS data within the timer interrupt, we increase our performance by not having extra code that isn't as necessary every interrupt.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

A nice advantage is that the design is fairly clean from the priority to MLFQS. We only need to add in the metadata with cpu load, recent cpu, niceness and the structure of our queue and list insert implies things work as desired. A disadvantage is that if we wanted to scale, things wouldn't be as nice because we'd have one gigantic queue - it might be good at that point ot split the giant queue into a few queues with ranges of priority lvels.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We used the library provided. In any case we'd probably have done it in a similar fashion because abstracting out the fixed point math makes life easier when storing data that is persistent through the program (cpu load, recent cpu). Moreover it makes things more readable when you know what is exactly happening with data flow.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

I wouldn't say anything was too easy. The priority donation was a little tedious because of all the list manipulation with C, in addition we overflowed the stack at some points and in other spots placing some code at some points could cause page faults. A bit of mysterious things...took a while to settle on the design. Luckily finishing this made MLFQS pretty straightforward.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Respect for people who write the schedulers, awareness of the issues with multithreading.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Tips to avoid stack overflow, more highlighting of dangerous places to put code.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

Not really, good guidance on this one.

>> Any other comments?
